{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2_model_id = \"gpt2\"\n",
    "bert_model_id = \"prajjwal1/bert-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_id, use_fast=True, add_prefix_space=False)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_id, use_fast=True, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence, tokenizer):\n",
    "    decoded_sequence = []\n",
    "    for token in sequence:\n",
    "        decoded_sequence.append(tokenizer.decode(token))\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizers(string, gpt2_tokenizer, bert_tokenizer):\n",
    "    print(f\"String: {string}\")\n",
    "    print(\" \")\n",
    "    encoded_string1 = gpt2_tokenizer.encode(string)\n",
    "    print(f\"Decoded sequence using GPT-2 tokenizer: {gpt2_tokenizer.decode(encoded_string1)}\")\n",
    "    print(f\"Decoded segmented sequence using GPT-2 tokenizer: {decode_sequence(encoded_string1, gpt2_tokenizer)}\")\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    encoded_string2 = bert_tokenizer.encode(string)\n",
    "    print(f\"Decoded sequence using BERT tokenizer: {bert_tokenizer.decode(encoded_string2)}\")\n",
    "    print(f\"Decoded segmented sequence using BERT tokenizer: {decode_sequence(encoded_string2, bert_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with \"medicine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizers(\"medicine\", gpt2_tokenizer, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with \"astrophysics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizers(\"astrophysics\", gpt2_tokenizer, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Guide Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_relm_resources.automata_examples.small_study_example import get_small_study_example\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "\n",
    "gpt2_property_model = get_small_study_example(SymbolStr(gpt2_tokenizer.eos_token))\n",
    "bert_property_model = get_small_study_example(SymbolStr(bert_tokenizer.sep_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=gpt2_tokenizer.eos_token_id).to(device)\n",
    "bert_model = AutoModelForCausalLM.from_pretrained(bert_model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=bert_tokenizer.sep_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from case_studies.gpt2.gpt2_probabilistic_model_wrapper import GPT2_probabilistic_model_wrapper\n",
    "from case_studies.bert_small.bert_small_probabilistic_model_wrapper import BERT_SMALL_probabilistic_model_wrapper\n",
    "from mini_relm_resources.automata_examples.small_study_example import alphabet\n",
    "\n",
    "gpt2_wrapper = GPT2_probabilistic_model_wrapper(50, alphabet, device, gpt2_model, gpt2_tokenizer)\n",
    "bert_wrapper = BERT_SMALL_probabilistic_model_wrapper(50, alphabet, device, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.utilities.guiding_wfa_sequence_generator import GuidingWDFASequenceGenerator\n",
    "gpt2_guiding_generator = GuidingWDFASequenceGenerator(gpt2_property_model, None)\n",
    "bert_guiding_generator = GuidingWDFASequenceGenerator(bert_property_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_exporters.dot_exporters.wfa_dot_exporting_strategy import WFADotExportingStrategy\n",
    "from IPython.display import display\n",
    "\n",
    "exporter = WFADotExportingStrategy()\n",
    "graph = exporter.create_graph(gpt2_property_model)\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = exporter.create_graph(bert_property_model)\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.syncronic_model_guided_language_model import SyncronicModelGuidedLanguageModel\n",
    "\n",
    "gpt2_syncrhronic_model = SyncronicModelGuidedLanguageModel(gpt2_wrapper, gpt2_property_model, model_name=\"GUIDED_GPT2\", max_seq_length=10, normalize_outputs=True, top_k=3)\n",
    "bert_syncrhronic_model = SyncronicModelGuidedLanguageModel(bert_wrapper, bert_property_model, model_name=\"GUIDED_BERT_SMALL\", max_seq_length=10, normalize_outputs=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.hypothesis_aware_sample_probabilistic_teacher import HypothesisAwareSampleProbabilisticTeacher\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pythautomata.utilities.probability_partitioner import QuantizationProbabilityPartitionerPlus\n",
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "partitioner = QuantizationProbabilityPartitionerPlus(1000)\n",
    "comparator = WFAPartitionComparator(partitioner)\n",
    "max_states = 50\n",
    "max_query_length = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_teacher = HypothesisAwareSampleProbabilisticTeacher(gpt2_syncrhronic_model, comparator, 30)\n",
    "bert_teacher = HypothesisAwareSampleProbabilisticTeacher(bert_syncrhronic_model, comparator, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, None, generate_partial_hipothesis = True, pre_cache_queries_for_building_hipothesis = True,  check_probabilistic_hipothesis = False, omit_zero_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_learning_result = learner.learn(gpt2_teacher, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_learning_result = learner.learn(bert_teacher, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(exporter.create_graph(gpt2_learning_result.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(exporter.create_graph(bert_learning_result.model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymodelextractor_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
